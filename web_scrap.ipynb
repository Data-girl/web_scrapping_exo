{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB SCRAPPING DE L'APEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests beautifulsoup4 html5lib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages():\n",
    "    '''Sert à sélectionner toutes les pages'''\n",
    "    urls = []\n",
    "    page = 75\n",
    "\n",
    "    for i in range(75,118):\n",
    "        i = f\"https://labonneboite.pole-emploi.fr/entreprises?j=%C3%89tudes+et+d%C3%A9veloppement+informatique&l=Paris+%2875%29&naf=&h=1&ij=&occupation=etudes-et-developpement-informatique&lat=&lon=&departments={page}\"\n",
    "        page += 1\n",
    "        urls.append(i)\n",
    "  \n",
    "    return urls\n",
    "\n",
    "# get_all_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entreprise():\n",
    "    '''Fonction qui extrait le nom d'une entreprises '''\n",
    "    # argument url permet d'itérer sur toutes les pages\n",
    "    # test = requests.get(\"https://labonneboite.pole-emploi.fr/entreprises?j=%C3%89tudes+et+d%C3%A9veloppement+informatique&l=Paris+%2875%29&naf=&h=1&ij=&occupation=etudes-et-developpement-informatique&lat=&lon=&departments=75\")\n",
    "    requete = requests.get(url)\n",
    "    \n",
    "    # Récupérer le contenu de la page test\n",
    "    soup = BeautifulSoup(requete.content, \"html.parser\")\n",
    "    # soup = BeautifulSoup(test.content, \"html.parser\")\n",
    "\n",
    "    # find_all tous les éléments\n",
    "    entreprises = soup.find_all('div', class_=\"lbb-bright-container lbb-result lbb-company\")\n",
    "    # print(len(entreprises)) #nbre d'entreprise sur chaque page\n",
    "\n",
    "    # Créer une liste vide\n",
    "    old_df_entreprise = []\n",
    "    for entreprise in entreprises:\n",
    "        # Créer un dictionnaire vide\n",
    "        df_entreprise = {}\n",
    "        try:\n",
    "            #find un seul élément\n",
    "\n",
    "            nom = entreprise.find('h3').text[:-11].strip()\n",
    "            df_entreprise['nom'] = entreprise.find('h3').text[:-11].strip()\n",
    "        except AttributeError as e:\n",
    "            df_entreprise['nom'] = \"\"\n",
    "        \n",
    "        try:\n",
    "            adresse = entreprise.find('span', class_=\"small\").text.strip()\n",
    "            adress_bis = re.sub(r\"\\s+\", \" \", adresse)\n",
    "            df_entreprise['adress_bis'] = re.sub(r\"\\s+\", \" \", adresse)\n",
    "        except AttributeError as e:\n",
    "            df_entreprise['adress_bis'] = \"\"\n",
    "        \n",
    "        try:\n",
    "            secteur = entreprise.find('h4', class_=\"company-naf-text\").text\n",
    "            df_entreprise['secteur'] = entreprise.find('h4', class_=\"company-naf-text\").text\n",
    "        except AttributeError as e:\n",
    "            df_entreprise['secteur'] = \"\"\n",
    "        \n",
    "        try:\n",
    "            nb_employes = entreprise.find('p').text\n",
    "            df_entreprise['nb_employes'] = entreprise.find('p').text\n",
    "        except AttributeError as e:\n",
    "            df_entreprise['nb_employes'] = \"\"\n",
    "        \n",
    "        try:\n",
    "            potentiel_embauche = entreprise.find('span', class_=\"rating-value\").text\n",
    "            df_entreprise['potentiel_embauche'] = entreprise.find('span', class_=\"rating-value\").text\n",
    "        except AttributeError as e:\n",
    "            df_entreprise['potentiel_embauche'] = \"\"\n",
    "        \n",
    "    \n",
    "        # sauvegarder les données dans une base de données\n",
    "        old_df_entreprise.append(df_entreprise)\n",
    "        print(old_df_entreprise)\n",
    "\n",
    "        # sauvegarder les données\n",
    "        chemin = r\"C:\\Users\\Jess\\Desktop\\Data Science courses\\webscrapping\\list_entreprises.txt\"\n",
    "        # chemin = r\"C:\\Users\\Jess\\Desktop\\Data Science courses\\webscrapping\\list_entreprises.csv\"\n",
    "        with open(chemin, \"a\") as f:\n",
    "            f.write(f\"{nom}\\n\")\n",
    "            f.write(f\"{adress_bis}\\n\")\n",
    "            f.write(f\"{secteur}\\n\")\n",
    "            f.write(f\"{nb_employes}\\n\")\n",
    "            f.write(f\"{potentiel_embauche}\\n\\n\")\n",
    "\n",
    "    df_final = pd.DataFrame(data=old_df_entreprise, \n",
    "                            columns= ['nom', 'adress_bis', 'secteur', 'nb_employes', 'potentiel_embauche'])\n",
    "\n",
    "        \n",
    "    df_final.to_csv('entreprise_name.csv')\n",
    "\n",
    "# parse_entreprise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data_per_page = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages():\n",
    "    '''Sert à sélectionner toutes les pages'''\n",
    "    urls = []\n",
    "    page = 75\n",
    "\n",
    "    for i in range(75,118):\n",
    "        i = f\"https://labonneboite.pole-emploi.fr/entreprises?j=%C3%89tudes+et+d%C3%A9veloppement+informatique&l=Paris+%2875%29&naf=&h=1&ij=&occupation=etudes-et-developpement-informatique&lat=&lon=&departments={page}\"\n",
    "        page += 1\n",
    "        urls.append(i)\n",
    "  \n",
    "    return urls\n",
    "\n",
    "# get_all_pages()\n",
    "\n",
    "\n",
    "def parse_entreprise(url):\n",
    "    '''Fonction qui extrait le nom d'une entreprises '''\n",
    "    # argument url permet d'itérer sur toutes les pages\n",
    "    # test = requests.get(\"https://labonneboite.pole-emploi.fr/entreprises?j=%C3%89tudes+et+d%C3%A9veloppement+informatique&l=Paris+%2875%29&naf=&h=1&ij=&occupation=etudes-et-developpement-informatique&lat=&lon=&departments=75\")\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Récupérer le contenu de la page test\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    # find_all tous les éléments\n",
    "    entreprises = soup.find_all('div', class_=\"lbb-bright-container lbb-result lbb-company\")\n",
    "    # print(len(entreprises)) #nbre d'entreprise sur chaque page\n",
    "\n",
    "   # Créer un dictionnaire vide\n",
    "    collect_data_per_page = dict()\n",
    "    \n",
    "    for entreprise in entreprises:\n",
    "        \n",
    "        try:\n",
    "            #find un seul élément\n",
    "            nom = entreprise.find('h3').text[:-11].strip()\n",
    "            \n",
    "        except AttributeError as e:\n",
    "            nom = \"\"\n",
    "        \n",
    "        try:\n",
    "            adresse = entreprise.find('span', class_=\"small\").text.strip()\n",
    "            adress_bis = re.sub(r\"\\s+\", \" \", adresse)\n",
    "            \n",
    "        except AttributeError as e:\n",
    "            adress_bis = \"\"\n",
    "        \n",
    "        try:\n",
    "            secteur = entreprise.find('h4', class_=\"company-naf-text\").text\n",
    "            \n",
    "        except AttributeError as e:\n",
    "            secteur = \"\"\n",
    "        \n",
    "        try:\n",
    "            nb_employes = entreprise.find('p').text\n",
    "    \n",
    "        except AttributeError as e:\n",
    "            nb_employes = \"\"\n",
    "        \n",
    "        try:\n",
    "            potentiel_embauche = entreprise.find('span', class_=\"rating-value\").text\n",
    "        \n",
    "        except AttributeError as e:\n",
    "            potentiel_embauche = \"\"\n",
    "        \n",
    "    \n",
    "        # sauvegarder les données dans une base de données (dictionnaire avec clé/valeur séparées d'un _)\n",
    "    \n",
    "        collect_data_per_page.update({f\"{nom}_{adress_bis}\": f\"{secteur}_{nb_employes}_{potentiel_embauche}\"})\n",
    "\n",
    "        df = pd.DataFrame(data=collect_data_per_page.items())\n",
    "        df[[\"nom\", \"adress_bis\"]] = df[0].str.split(\"_\", expand=True)\n",
    "        df[[\"secteur\", \"nb_employes\", \"potentiel_embauche\"]] = df[1].str.split(\"_\", expand=True)\n",
    "        df.drop(columns=[0,1], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def parse_all_entreprises():\n",
    "    '''Permet d'appeler la premiere fonction, en récupérant toutes les urls '''\n",
    "    \n",
    "    pages = get_all_pages()\n",
    "    # Appeler la deuxieme fonction dans la boucle for\n",
    "    for page in pages:\n",
    "        parse_entreprise(url=page)\n",
    "        # print(f\"On scrappe la {page[-3:]}\")\n",
    "\n",
    "\n",
    "\n",
    "parse_all_entreprises()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom</th>\n",
       "      <th>adress_bis</th>\n",
       "      <th>secteur</th>\n",
       "      <th>nb_employes</th>\n",
       "      <th>potentiel_embauche</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>NOVATEC GUADELOUPE\\n    - BA</td>\n",
       "      <td>BAIE-MAHAULT</td>\n",
       "      <td>Autres activités informatiques</td>\n",
       "      <td>6 à 9 salariés</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>INSTITUT PASTEUR DE GUYANE</td>\n",
       "      <td>CAYENNE</td>\n",
       "      <td>Recherche-développement en autres sciences phy...</td>\n",
       "      <td>50 à 99 salariés</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>SOCIETE DE RECYCLAGE ET DE RECONDITIONNE\\n    - S</td>\n",
       "      <td>SAINT-DENIS</td>\n",
       "      <td>Commerce de détail d'ordinateurs, d'unités pér...</td>\n",
       "      <td>6 à 9 salariés</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>BEELIZ\\n    - BA</td>\n",
       "      <td>BAIE-MAHAULT</td>\n",
       "      <td>Programmation informatique</td>\n",
       "      <td>0 salarié</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>AVANT-GARDE OUTRE MER</td>\n",
       "      <td>CAYENNE</td>\n",
       "      <td>Tierce maintenance de systèmes et d’applicatio...</td>\n",
       "      <td>0 salarié</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   nom    adress_bis  \\\n",
       "432                       NOVATEC GUADELOUPE\\n    - BA  BAIE-MAHAULT   \n",
       "433                         INSTITUT PASTEUR DE GUYANE       CAYENNE   \n",
       "434  SOCIETE DE RECYCLAGE ET DE RECONDITIONNE\\n    - S   SAINT-DENIS   \n",
       "435                                   BEELIZ\\n    - BA  BAIE-MAHAULT   \n",
       "436                              AVANT-GARDE OUTRE MER       CAYENNE   \n",
       "\n",
       "                                               secteur       nb_employes  \\\n",
       "432                     Autres activités informatiques    6 à 9 salariés   \n",
       "433  Recherche-développement en autres sciences phy...  50 à 99 salariés   \n",
       "434  Commerce de détail d'ordinateurs, d'unités pér...    6 à 9 salariés   \n",
       "435                         Programmation informatique         0 salarié   \n",
       "436  Tierce maintenance de systèmes et d’applicatio...         0 salarié   \n",
       "\n",
       "    potentiel_embauche  \n",
       "432                2.8  \n",
       "433                2.7  \n",
       "434                2.7  \n",
       "435                3.6  \n",
       "436                3.1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=collect_data_per_page.items())\n",
    "df[[\"nom\", \"adress_bis\"]] = df[0].str.split(\"_\", expand=True)\n",
    "df[[\"secteur\", \"nb_employes\", \"potentiel_embauche\"]] = df[1].str.split(\"_\", expand=True)\n",
    "df.drop(columns=[0,1], axis=1, inplace=True)\n",
    "df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
